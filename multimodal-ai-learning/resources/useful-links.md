# 유용한 링크 모음

> 멀티모달 AI 학습을 위한 튜토리얼, 블로그, 강의 자료

## 공식 문서 및 튜토리얼

### Hugging Face

| 링크 | 설명 |
|------|------|
| [Vision Transformers Explained](https://huggingface.co/blog/vision-transformers) | ViT 상세 설명 |
| [VLMs Explained](https://huggingface.co/blog/vlms) | Vision Language Models 소개 |
| [VLMs 2025](https://huggingface.co/blog/vlms-2025) | 2025년 VLM 동향 |
| [ViT Documentation](https://huggingface.co/docs/transformers/en/model_doc/vit) | Hugging Face ViT 사용법 |
| [CLIP Documentation](https://huggingface.co/docs/transformers/en/model_doc/clip) | Hugging Face CLIP 사용법 |

### 공식 프로젝트

| 링크 | 설명 |
|------|------|
| [OpenAI CLIP](https://openai.com/index/clip/) | CLIP 공식 소개 |
| [LLaVA Official](https://llava-vl.github.io/) | LLaVA 프로젝트 페이지 |
| [Qwen-VL](https://qwenlm.github.io/blog/qwen2-vl/) | Qwen-VL 공식 블로그 |

---

## 학습 플랫폼

### 강의 및 코스

| 링크 | 설명 |
|------|------|
| [Stanford CS231n](http://cs231n.stanford.edu/) | 컴퓨터 비전 기초 (ViT 포함) |
| [Stanford CS224N](https://web.stanford.edu/class/cs224n/) | NLP + Transformer 기초 |
| [Dive into Deep Learning](https://d2l.ai/) | 무료 딥러닝 교과서 (ViT 챕터 포함) |
| [Fast.ai](https://www.fast.ai/) | 실용적 딥러닝 강의 |

### 인터랙티브 튜토리얼

| 링크 | 설명 |
|------|------|
| [Pinecone - Vision Transformers](https://www.pinecone.io/learn/series/image-search/vision-transformers/) | ViT 시각적 설명 |
| [Jay Alammar's Blog](https://jalammar.github.io/) | Transformer 시각화 |
| [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) | Transformer 직관적 이해 |

---

## 기술 블로그

### 한국어 자료

| 링크 | 설명 |
|------|------|
| [Hugging Face 한국어 블로그](https://huggingface.co/blog?tag=korean) | 한국어 기술 문서 |
| [카카오 기술 블로그 - AI](https://tech.kakao.com/posts?category=ai) | AI 관련 기술 글 |
| [네이버 클로바 블로그](https://clova.ai/ko/research) | 네이버 AI 연구 |

### 영어 자료

| 링크 | 설명 |
|------|------|
| [Lilian Weng's Blog](https://lilianweng.github.io/) | OpenAI 연구자 블로그, 심층 분석 |
| [The AI Summer](https://theaisummer.com/) | 딥러닝 튜토리얼 |
| [Roboflow Blog](https://blog.roboflow.com/) | 컴퓨터 비전 실용 가이드 |
| [Ultralytics Blog](https://www.ultralytics.com/blog) | CV 최신 동향 |

---

## GitHub 리포지토리

### Awesome Lists

| 링크 | 설명 |
|------|------|
| [Awesome-VLM-Architectures](https://github.com/gokayfem/awesome-vlm-architectures) | VLM 아키텍처 총정리 |
| [Awesome-Multimodal-LLMs](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) | 멀티모달 LLM 모음 |
| [Awesome-Unified-Multimodal](https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models) | 통합 멀티모달 모델 |
| [Awesome-Vision-Language](https://github.com/sangminwoo/awesome-vision-language-pretraining) | VLP 논문/코드 모음 |

### 구현 및 코드

| 링크 | 설명 |
|------|------|
| [OpenCLIP](https://github.com/mlfoundations/open_clip) | CLIP 오픈소스 재구현 |
| [timm (PyTorch Image Models)](https://github.com/huggingface/pytorch-image-models) | ViT 등 이미지 모델 구현 |
| [LLaVA](https://github.com/haotian-liu/LLaVA) | 핵심 VLM 구현 |
| [Open-LLaVA-NeXT](https://github.com/xiaoachen98/Open-LLaVA-NeXT) | LLaVA-NeXT 재현 |

---

## 벤치마크 및 데이터셋

### 벤치마크

| 링크 | 설명 |
|------|------|
| [MMMU](https://mmmu-benchmark.github.io/) | 대학 수준 멀티모달 이해 |
| [MMBench](https://opencompass.org.cn/MMBench) | 종합 VLM 평가 |
| [SEED-Bench](https://github.com/AILab-CVC/SEED-Bench) | 이미지+비디오 평가 |
| [VQAv2](https://visualqa.org/) | 시각 질의응답 벤치마크 |

### 데이터셋

| 링크 | 설명 |
|------|------|
| [LAION-5B](https://laion.ai/blog/laion-5b/) | 대규모 이미지-텍스트 데이터셋 |
| [CC3M/CC12M](https://ai.google.com/research/ConceptualCaptions/) | Conceptual Captions |
| [Visual Genome](https://visualgenome.org/) | 장면 그래프, 관계 데이터 |

---

## 모델 허브

### 사전학습 모델 다운로드

| 링크 | 설명 |
|------|------|
| [Hugging Face Hub](https://huggingface.co/models) | 가장 큰 모델 허브 |
| [OpenCLIP Models](https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv) | 다양한 CLIP 모델 |
| [timm Models](https://huggingface.co/timm) | 이미지 모델 모음 |

### 주요 모델 링크

| 모델 | Hugging Face 링크 |
|------|-------------------|
| ViT-Base | [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224) |
| CLIP | [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) |
| LLaVA-1.5 | [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) |
| Qwen2-VL | [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) |

---

## 커뮤니티

### 포럼 및 디스커션

| 링크 | 설명 |
|------|------|
| [Hugging Face Forums](https://discuss.huggingface.co/) | 모델 사용 질문 |
| [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) | ML 커뮤니티 |
| [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/) | 로컬 LLM 커뮤니티 (VLM 포함) |

### Discord / Slack

| 링크 | 설명 |
|------|------|
| [Hugging Face Discord](https://discord.gg/huggingface) | HF 공식 디스코드 |
| [PyTorch Discord](https://discord.gg/pytorch) | PyTorch 커뮤니티 |

---

## 뉴스 및 최신 동향

| 링크 | 설명 |
|------|------|
| [Papers with Code](https://paperswithcode.com/) | 최신 논문 + 코드 |
| [arXiv Sanity](https://arxiv-sanity-lite.com/) | arXiv 논문 큐레이션 |
| [Hugging Face Papers](https://huggingface.co/papers) | 일일 논문 추천 |
| [The Gradient](https://thegradient.pub/) | AI 심층 분석 |
| [AI News](https://newsletter.theaiedge.io/) | AI 뉴스레터 |

---

## 도구 및 유틸리티

### 시각화 및 해석

| 링크 | 설명 |
|------|------|
| [Netron](https://netron.app/) | 모델 아키텍처 시각화 |
| [Attention Viz](https://github.com/hila-chefer/Transformer-Explainability) | Transformer Attention 시각화 |
| [CLIP Explorer](https://huggingface.co/spaces/CLIP-Explainability/clip-explainability) | CLIP 작동 탐색 |

### 실험 및 추적

| 링크 | 설명 |
|------|------|
| [Weights & Biases](https://wandb.ai/) | 실험 추적 |
| [MLflow](https://mlflow.org/) | ML 라이프사이클 관리 |
| [TensorBoard](https://www.tensorflow.org/tensorboard) | 학습 시각화 |

---

## 빠른 시작 가이드

### 처음 시작하는 분

1. [Jay Alammar - Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) 읽기
2. [Pinecone ViT 설명](https://www.pinecone.io/learn/series/image-search/vision-transformers/) 보기
3. [Hugging Face VLM 블로그](https://huggingface.co/blog/vlms) 읽기
4. [LLaVA 데모](https://llava.hliu.cc/) 체험하기

### 실습하고 싶은 분

1. [Hugging Face ViT 튜토리얼](https://huggingface.co/docs/transformers/en/model_doc/vit) 따라하기
2. [CLIP Colab 노트북](https://colab.research.google.com/github/openai/clip/blob/main/notebooks/Interacting_with_CLIP.ipynb) 실행
3. [LLaVA Quick Start](https://github.com/haotian-liu/LLaVA#quick-start) 따라하기

---

*마지막 업데이트: 2025-12-20*
